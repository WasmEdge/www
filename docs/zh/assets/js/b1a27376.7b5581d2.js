"use strict";(self.webpackChunkbook=self.webpackChunkbook||[]).push([[1428],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>h});var n=a(67294);function l(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){l(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},m=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,l=e.mdxType,r=e.originalType,s=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),u=p(a),d=l,h=u["".concat(s,".").concat(d)]||u[d]||c[d]||r;return a?n.createElement(h,o(o({ref:t},m),{},{components:a})):n.createElement(h,o({ref:t},m))}));function h(e,t){var a=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=a.length,o=new Array(r);o[0]=d;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[u]="string"==typeof e?e:l,o[1]=i;for(var p=2;p<r;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},13100:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>i,toc:()=>p});var n=a(87462),l=(a(67294),a(3905));const r={sidebar_position:1},o="Llama 2 inference",i={unversionedId:"develop/rust/wasinn/llm_inference",id:"develop/rust/wasinn/llm_inference",title:"Llama 2 inference",description:"WasmEdge now supports running llama2 series of models in Rust. We will use this example project to show how to make AI inferences with the llama2 model in WasmEdge and Rust.",source:"@site/i18n/zh/docusaurus-plugin-content-docs/current/develop/rust/wasinn/llm_inference.md",sourceDirName:"develop/rust/wasinn",slug:"/develop/rust/wasinn/llm_inference",permalink:"/docs/zh/develop/rust/wasinn/llm_inference",draft:!1,editUrl:"https://github.com/wasmedge/docs/blob/main/docs/develop/rust/wasinn/llm_inference.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"developSidebar",previous:{title:"AI inference",permalink:"/docs/zh/category/ai-inference"},next:{title:"Mediapipe solutions",permalink:"/docs/zh/develop/rust/wasinn/mediapipe"}},s={},p=[{value:"Prerequisite",id:"prerequisite",level:2},{value:"Quick start",id:"quick-start",level:2},{value:"Build and run",id:"build-and-run",level:2},{value:"Options",id:"options",level:2},{value:"Improving performance",id:"improving-performance",level:2},{value:"Understand the code",id:"understand-the-code",level:2},{value:"Resources",id:"resources",level:2}],m={toc:p},u="wrapper";function c(e){let{components:t,...a}=e;return(0,l.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"llama-2-inference"},"Llama 2 inference"),(0,l.kt)("p",null,"WasmEdge now supports running llama2 series of models in Rust. We will use ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/second-state/LlamaEdge/tree/main/chat"},"this example project")," to show how to make AI inferences with the llama2 model in WasmEdge and Rust."),(0,l.kt)("p",null,"WasmEdge now supports the following models:"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Llama-2-7B-Chat"),(0,l.kt)("li",{parentName:"ol"},"Llama-2-13B-Chat"),(0,l.kt)("li",{parentName:"ol"},"CodeLlama-13B-Instruct"),(0,l.kt)("li",{parentName:"ol"},"Mistral-7B-Instruct-v0.1"),(0,l.kt)("li",{parentName:"ol"},"Mistral-7B-Instruct-v0.2"),(0,l.kt)("li",{parentName:"ol"},"MistralLite-7B"),(0,l.kt)("li",{parentName:"ol"},"OpenChat-3.5-0106"),(0,l.kt)("li",{parentName:"ol"},"OpenChat-3.5-1210"),(0,l.kt)("li",{parentName:"ol"},"OpenChat-3.5"),(0,l.kt)("li",{parentName:"ol"},"Wizard-Vicuna-13B-Uncensored-GGUF"),(0,l.kt)("li",{parentName:"ol"},"TinyLlama-1.1B-Chat-v1.0"),(0,l.kt)("li",{parentName:"ol"},"Baichuan2-13B-Chat"),(0,l.kt)("li",{parentName:"ol"},"OpenHermes-2.5-Mistral-7B"),(0,l.kt)("li",{parentName:"ol"},"Dolphin-2.2-Yi-34B"),(0,l.kt)("li",{parentName:"ol"},"Dolphin-2.6-Mistral-7B"),(0,l.kt)("li",{parentName:"ol"},"Samantha-1.2-Mistral-7B"),(0,l.kt)("li",{parentName:"ol"},"Samantha-1.11-CodeLlama-34B"),(0,l.kt)("li",{parentName:"ol"},"WizardCoder-Python-7B-V1.0"),(0,l.kt)("li",{parentName:"ol"},"Zephyr-7B-Alpha"),(0,l.kt)("li",{parentName:"ol"},"WizardLM-13B-V1.0-Uncensored"),(0,l.kt)("li",{parentName:"ol"},"Orca-2-13B"),(0,l.kt)("li",{parentName:"ol"},"Neural-Chat-7B-v3-1"),(0,l.kt)("li",{parentName:"ol"},"Yi-34B-Chat"),(0,l.kt)("li",{parentName:"ol"},"Starling-LM-7B-alpha"),(0,l.kt)("li",{parentName:"ol"},"DeepSeek-Coder-6.7B"),(0,l.kt)("li",{parentName:"ol"},"DeepSeek-LLM-7B-Chat"),(0,l.kt)("li",{parentName:"ol"},"SOLAR-10.7B-Instruct-v1.0"),(0,l.kt)("li",{parentName:"ol"},"Mixtral-8x7B-Instruct-v0.1"),(0,l.kt)("li",{parentName:"ol"},"Nous-Hermes-2-Mixtral-8x7B-DPO"),(0,l.kt)("li",{parentName:"ol"},"Nous-Hermes-2-Mixtral-8x7B-SFT")),(0,l.kt)("p",null,"And more, please check ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/second-state/LlamaEdge/blob/main/models.md"},"the supported models")," for detials."),(0,l.kt)("h2",{id:"prerequisite"},"Prerequisite"),(0,l.kt)("p",null,"Besides the ",(0,l.kt)("a",{parentName:"p",href:"/docs/zh/develop/rust/setup"},"regular WasmEdge and Rust requirements"),", please make sure that you have the ",(0,l.kt)("a",{parentName:"p",href:"/docs/zh/start/install#wasi-nn-plug-in-with-ggml-backend"},"Wasi-NN plugin with ggml installed"),"."),(0,l.kt)("h2",{id:"quick-start"},"Quick start"),(0,l.kt)("p",null,"Because the example already includes a compiled WASM file from the Rust code, we could use WasmEdge CLI to execute the example directly."),(0,l.kt)("p",null,"First, get the latest llama-chat wasm application"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm\n")),(0,l.kt)("p",null,"Next, let's get the model. In this example, we are going to use the llama2 7b chat model in GGUF format. You can also use other kinds of llama2 models, check out ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/second-state/llamaedge/blob/main/chat/README.md#get-model"},"here"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"curl -LO https://huggingface.co/wasmedge/llama2/resolve/main/llama-2-7b-chat-q5_k_m.gguf\n")),(0,l.kt)("p",null,"Run the inference application in WasmEdge."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat-q5_k_m.gguf llama-chat.wasm\n")),(0,l.kt)("p",null,"After executing the command, you may need to wait a moment for the input prompt to appear. You can enter your question once you see the ",(0,l.kt)("inlineCode",{parentName:"p"},"[USER]:")," prompt:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"[USER]:\nI have two apples, each costing 5 dollars. What is the total cost of these apple\n[ASSISTANT]:\nThe total cost of the two apples is 10 dollars.\n[USER]:\nHow about four apples?\n[ASSISTANT]:\nThe total cost of four apples is 20 dollars.\n")),(0,l.kt)("h2",{id:"build-and-run"},"Build and run"),(0,l.kt)("p",null,"Let's build the wasm file from the rust source code. First, git clone the ",(0,l.kt)("inlineCode",{parentName:"p"},"llamaedge")," repo."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/LlamaEdge/LlamaEdge.git\ncd chat\n")),(0,l.kt)("p",null,"Second, use ",(0,l.kt)("inlineCode",{parentName:"p"},"cargo")," to build the example project."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"cargo build --target wasm32-wasi --release\n")),(0,l.kt)("p",null,"The output WASM file is ",(0,l.kt)("inlineCode",{parentName:"p"},"target/wasm32-wasi/release/llama-chat.wasm"),". Next, use WasmEdge to load the llama-2-7b model and then ask the model to questions."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat-q5_k_m.gguf llama-chat.wasm\n")),(0,l.kt)("p",null,"After executing the command, you may need to wait a moment for the input prompt to appear. You can enter your question once you see the ",(0,l.kt)("inlineCode",{parentName:"p"},"[USER]:")," prompt:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"[USER]:\nWho is Robert Oppenheimer?\n[ASSISTANT]:\nRobert Oppenheimer was an American theoretical physicist and director of the Manhattan Project, which developed the atomic bomb during World War II. He is widely regarded as one of the most important physicists of the 20th century and is known for his contributions to the development of quantum mechanics and the theory of the atomic nucleus. Oppenheimer was also a prominent figure in the post-war nuclear weapons debate and was a strong advocate for international cooperation on nuclear weapons control.\n")),(0,l.kt)("h2",{id:"options"},"Options"),(0,l.kt)("p",null,"You can configure the chat inference application through CLI options."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'  -m, --model-alias <ALIAS>\n          Model alias [default: default]\n  -c, --ctx-size <CTX_SIZE>\n          Size of the prompt context [default: 4096]\n  -n, --n-predict <N_PRDICT>\n          Number of tokens to predict [default: 1024]\n  -g, --n-gpu-layers <N_GPU_LAYERS>\n          Number of layers to run on the GPU [default: 100]\n  -b, --batch-size <BATCH_SIZE>\n          Batch size for prompt processing [default: 4096]\n  -r, --reverse-prompt <REVERSE_PROMPT>\n          Halt generation at PROMPT, return control.\n  -s, --system-prompt <SYSTEM_PROMPT>\n          System prompt message string [default: "[Default system message for the prompt template]"]\n  -p, --prompt-template <TEMPLATE>\n          Prompt template. [default: llama-2-chat] [possible values: llama-2-chat, codellama-instruct, mistral-instruct-v0.1, mistrallite, openchat, belle-llama-2-chat, vicuna-chat, chatml]\n      --log-prompts\n          Print prompt strings to stdout\n      --log-stat\n          Print statistics to stdout\n      --log-all\n          Print all log information to stdout\n      --stream-stdout\n          Print the output to stdout in the streaming way\n  -h, --help\n          Print help\n')),(0,l.kt)("p",null,"The ",(0,l.kt)("inlineCode",{parentName:"p"},"--prompt-template")," option is perhaps the most interesting. It allows the application to support different open source LLM models beyond llama2. Check out more prompt templates ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/LlamaEdge/LlamaEdge/tree/main/api-server/chat-prompts"},"here"),"."),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Template name"),(0,l.kt)("th",{parentName:"tr",align:null},"Model"),(0,l.kt)("th",{parentName:"tr",align:null},"Download"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"llama-2-chat"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://ai.meta.com/llama/"},"The standard llama2 chat model")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/wasmedge/llama2/resolve/main/llama-2-7b-chat-q5_k_m.gguf"},"7b"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"codellama-instruct"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://about.fb.com/news/2023/08/code-llama-ai-for-coding/"},"CodeLlama")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q5_K_M.gguf"},"7b"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"mistral-instruct-v0.1"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://mistral.ai/"},"Mistral")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf"},"7b"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"mistrallite"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/amazon/MistralLite"},"Mistral Lite")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/TheBloke/MistralLite-7B-GGUF/resolve/main/mistrallite.Q5_K_M.gguf"},"7b"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"openchat"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://github.com/imoneoi/openchat"},"OpenChat")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/TheBloke/openchat_3.5-GGUF/resolve/main/openchat_3.5.Q5_K_M.gguf"},"7b"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"belle-llama-2-chat"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://github.com/LianjiaTech/BELLE"},"BELLE")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/second-state/BELLE-Llama2-13B-Chat-0.4M-GGUF/resolve/main/BELLE-Llama2-13B-Chat-0.4M-ggml-model-q4_0.gguf"},"13b"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"vicuna-chat"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://lmsys.org/blog/2023-03-30-vicuna/"},"Vicuna")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/TheBloke/vicuna-7B-v1.5-GGUF/resolve/main/vicuna-7b-v1.5.Q5_K_M.gguf"},"7b"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"chatml"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/chargoddard/rpguild-chatml-13b"},"ChatML")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/TheBloke/rpguild-chatml-13B-GGUF/resolve/main/rpguild-chatml-13b.Q5_K_M.gguf"},"13b"))))),(0,l.kt)("p",null,"Furthermore, the following command tells WasmEdge to print out logs and statistics of the model at runtime."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat-q5_k_m.gguf \\\n  llama-chat.wasm --prompt-template llama-2-chat --log-stat\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model: kv self size  =  256.00 MB\nllama_new_context_with_model: compute buffer total size = 76.63 MB\n[2023-11-07 02:07:44.019] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n\nllama_print_timings:        load time =   11523.19 ms\nllama_print_timings:      sample time =       2.62 ms /   102 runs   (    0.03 ms per token, 38961.04 tokens per second)\nllama_print_timings: prompt eval time =   11479.27 ms /    49 tokens (  234.27 ms per token,     4.27 tokens per second)\nllama_print_timings:        eval time =   13571.37 ms /   101 runs   (  134.37 ms per token,     7.44 tokens per second)\nllama_print_timings:       total time =   25104.57 ms\n[ASSISTANT]:\nAh, a fellow Peanuts enthusiast! Snoopy is Charlie Brown's lovable and imaginative beagle, known for his wild and wacky adventures in the comic strip and television specials. He's a loyal companion to Charlie Brown and the rest of the Peanuts gang, and his antics often provide comic relief in the series. Is there anything else you'd like to know about Snoopy? \ud83d\udc36\n")),(0,l.kt)("h2",{id:"improving-performance"},"Improving performance"),(0,l.kt)("p",null,"You can make the inference program run faster by AOT compiling the wasm file first."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"wasmedge compile llama-chat.wasm llama-chat.wasm\nwasmedge --dir .:.  --nn-preload default:GGML:AUTO:llama-2-7b-chat-q5_k_m.gguf llama-chat.wasm\n")),(0,l.kt)("h2",{id:"understand-the-code"},"Understand the code"),(0,l.kt)("p",null,"The ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/second-state/llamaedge/blob/main/chat/src/main.rs"},"main.rs")," is the full Rust code to create an interactive chatbot using a LLM. The Rust program manages the user input, tracks the conversation history, transforms the text into the llama2 and other model\u2019s chat templates, and runs the inference operations using the WASI NN standard API. The code logic for the chat interaction is somewhat complex. In this section, we will use the ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/second-state/llamaedge/tree/main/simple"},"simple example")," to explain how to set up and perform one inference round trip. Here is how you use the simple example."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"# Download the compiled simple inference wasm\ncurl -LO https://github.com/second-state/llamaedge/releases/latest/download/llama-simple.wasm\n\n# Give it a prompt and ask it to use the model to complete it.\nwasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat-q5_k_m.gguf llama-simple.wasm \\\n  --prompt 'Robert Oppenheimer most important achievement is ' --ctx-size 4096\n\noutput: in 1942, when he led the team that developed the first atomic bomb, which was dropped on Hiroshima, Japan in 1945.\n")),(0,l.kt)("p",null,"First, let's parse command line arguments to customize the chatbot's behavior using ",(0,l.kt)("inlineCode",{parentName:"p"},"Command")," struct. It extracts the following parameters: ",(0,l.kt)("inlineCode",{parentName:"p"},"prompt")," (a prompt that guides the conversation), ",(0,l.kt)("inlineCode",{parentName:"p"},"model_alias")," (a list for the loaded model), and ",(0,l.kt)("inlineCode",{parentName:"p"},"ctx_size")," (the size of the chat context)."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-rust"},'fn main() -> Result<(), String> {\n    let matches = Command::new("Simple LLM inference")\n        .arg(\n            Arg::new("prompt")\n                .short(\'p\')\n                .long("prompt")\n                .value_name("PROMPT")\n                .help("Sets the prompt.")\n                .required(true),\n        )\n        .arg(\n            Arg::new("model_alias")\n                .short(\'m\')\n                .long("model-alias")\n                .value_name("ALIAS")\n                .help("Sets the model alias")\n                .default_value("default"),\n        )\n        .arg(\n            Arg::new("ctx_size")\n                .short(\'c\')\n                .long("ctx-size")\n                .value_parser(clap::value_parser!(u32))\n                .value_name("CTX_SIZE")\n                .help("Sets the prompt context size")\n                .default_value(DEFAULT_CTX_SIZE),\n        )\n        .get_matches();\n\n    // model alias\n    let model_name = matches\n        .get_one::<String>("model_alias")\n        .unwrap()\n        .to_string();\n\n    // prompt context size\n    let ctx_size = matches.get_one::<u32>("ctx_size").unwrap();\n    CTX_SIZE\n        .set(*ctx_size as usize)\n        .expect("Fail to parse prompt context size");\n\n    // prompt\n    let prompt = matches.get_one::<String>("prompt").unwrap().to_string();\n')),(0,l.kt)("p",null,"After that, the program will create a new Graph using the ",(0,l.kt)("inlineCode",{parentName:"p"},"GraphBuilder")," and loads the model specified by the ",(0,l.kt)("inlineCode",{parentName:"p"},"model_name")," ."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-rust"},'// load the model to wasi-nn\n     let graph =\n        wasi_nn::GraphBuilder::new(wasi_nn::GraphEncoding::Ggml, wasi_nn::ExecutionTarget::AUTO)\n            .build_from_cache(&model_name)\n            .expect("Failed to load the model");\n')),(0,l.kt)("p",null,"Next, We create an execution context from the loaded Graph. The context is mutable because we will be changing it when we set the input tensor and execute the inference."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-rust"},' // initialize the execution context\n    let mut context = graph\n        .init_execution_context()\n        .expect("Failed to init context");\n')),(0,l.kt)("p",null,"Next, The prompt is converted into bytes and set as the input tensor for the model inference."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-rust"},' // set input tensor\n    let tensor_data = prompt.as_str().as_bytes().to_vec();\n    context\n        .set_input(0, wasi_nn::TensorType::U8, &[1], &tensor_data)\n        .expect("Failed to set prompt as the input tensor");\n')),(0,l.kt)("p",null,"Next, execute the model inference."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-rust"},'  // execute the inference\n    context.compute().expect("Failed to complete inference");\n')),(0,l.kt)("p",null,"After the inference is finished, extract the result from the computation context and losing invalid UTF8 sequences handled by converting the output to a string using ",(0,l.kt)("inlineCode",{parentName:"p"},"String::from_utf8_lossy"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-rust"},'  let mut output_buffer = vec![0u8; *CTX_SIZE.get().unwrap()];\n    let mut output_size = context\n        .get_output(0, &mut output_buffer)\n        .expect("Failed to get output tensor");\n    output_size = std::cmp::min(*CTX_SIZE.get().unwrap(), output_size);\n    let output = String::from_utf8_lossy(&output_buffer[..output_size]).to_string();\n')),(0,l.kt)("p",null,"Finally, print the prompt and the inference output to the console."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-rust"},'println!("\\nprompt: {}", &prompt);\nprintln!("\\noutput: {}", output);\n')),(0,l.kt)("h2",{id:"resources"},"Resources"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"If you're looking for multi-turn conversations with llama 2 models, please check out the above mentioned chat example source code ",(0,l.kt)("a",{parentName:"li",href:"https://github.com/second-state/llamaedge/tree/main/chat"},"here"),"."),(0,l.kt)("li",{parentName:"ul"},"If you want to construct OpenAI-compatible APIs specifically for your llama2 model, or the Llama2 model itself, please check out the source code ",(0,l.kt)("a",{parentName:"li",href:"https://github.com/second-state/llamaedge/tree/main/api-server"},"for the API server"),"."),(0,l.kt)("li",{parentName:"ul"},"To learn more, please check out ",(0,l.kt)("a",{parentName:"li",href:"https://medium.com/stackademic/fast-and-portable-llama2-inference-on-the-heterogeneous-edge-a62508e82359"},"this article"),".")))}c.isMDXComponent=!0}}]);