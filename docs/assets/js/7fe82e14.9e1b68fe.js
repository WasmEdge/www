"use strict";(self.webpackChunkbook=self.webpackChunkbook||[]).push([[2635],{3905:(e,t,a)=>{a.d(t,{Zo:()=>s,kt:()=>N});var n=a(67294);function l(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){l(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var d=n.createContext({}),p=function(e){var t=n.useContext(d),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},s=function(e){var t=p(e.components);return n.createElement(d.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,l=e.mdxType,i=e.originalType,d=e.parentName,s=o(e,["components","mdxType","originalType","parentName"]),u=p(a),c=l,N=u["".concat(d,".").concat(c)]||u[c]||m[c]||i;return a?n.createElement(N,r(r({ref:t},s),{},{components:a})):n.createElement(N,r({ref:t},s))}));function N(e,t){var a=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var i=a.length,r=new Array(i);r[0]=c;var o={};for(var d in t)hasOwnProperty.call(t,d)&&(o[d]=t[d]);o.originalType=e,o[u]="string"==typeof e?e:l,r[1]=o;for(var p=2;p<i;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},1480:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var n=a(87462),l=(a(67294),a(3905));const i={sidebar_position:2},r="Build with WASI-NN Plug-in",o={unversionedId:"contribute/source/plugin/wasi_nn",id:"contribute/source/plugin/wasi_nn",title:"Build with WASI-NN Plug-in",description:"The WASI-NN plug-in is a proposed WebAssembly System Interface (WASI) API for machine learning. It allows WebAssembly programs to access host-provided machine learning functions.",source:"@site/docs/contribute/source/plugin/wasi_nn.md",sourceDirName:"contribute/source/plugin",slug:"/contribute/source/plugin/wasi_nn",permalink:"/docs/contribute/source/plugin/wasi_nn",draft:!1,editUrl:"https://github.com/wasmedge/docs/blob/main/docs/contribute/source/plugin/wasi_nn.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"contributeSidebar",previous:{title:"Build WasmEdge With WASI-Logging Plug-in",permalink:"/docs/contribute/source/plugin/wasi_logging"},next:{title:"Build WasmEdge With WasmEdge-Process Plug-in",permalink:"/docs/contribute/source/plugin/process"}},d={},p=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Build WasmEdge with WASI-NN OpenVINO Backend",id:"build-wasmedge-with-wasi-nn-openvino-backend",level:2},{value:"Build WasmEdge with WASI-NN PyTorch Backend",id:"build-wasmedge-with-wasi-nn-pytorch-backend",level:2},{value:"Build WasmEdge with WASI-NN TensorFlow-Lite Backend",id:"build-wasmedge-with-wasi-nn-tensorflow-lite-backend",level:2},{value:"Build WasmEdge with WASI-NN llama.cpp Backend",id:"build-wasmedge-with-wasi-nn-llamacpp-backend",level:2},{value:"Build with llama.cpp Backend on MacOS",id:"build-with-llamacpp-backend-on-macos",level:3},{value:"Intel Model",id:"intel-model",level:4},{value:"Apple Silicon Model",id:"apple-silicon-model",level:4},{value:"Build with llama.cpp Backend on Linux",id:"build-with-llamacpp-backend-on-linux",level:3},{value:"Ubuntu/Debian with CUDA 12",id:"ubuntudebian-with-cuda-12",level:4},{value:"Ubuntu on NVIDIA Jetson AGX Orin",id:"ubuntu-on-nvidia-jetson-agx-orin",level:4},{value:"Ubuntu/Debian with OpenBLAS",id:"ubuntudebian-with-openblas",level:4},{value:"General Linux without any acceleration framework",id:"general-linux-without-any-acceleration-framework",level:4},{value:"Build with llama.cpp Backend on Windows",id:"build-with-llamacpp-backend-on-windows",level:3},{value:"Install Dependencies for llama.cpp And Build on Windows",id:"install-dependencies-for-llamacpp-and-build-on-windows",level:4},{value:"Execute the WASI-NN plugin with the llama example on Windows",id:"execute-the-wasi-nn-plugin-with-the-llama-example-on-windows",level:4},{value:"Troubleshooting: AMD Radeon Integrated Graphics (Windows)",id:"troubleshooting-amd-radeon-integrated-graphics-windows",level:4},{value:"Appendix for llama.cpp backend",id:"appendix-for-llamacpp-backend",level:3},{value:"Build WasmEdge with WASI-NN Piper Backend",id:"build-wasmedge-with-wasi-nn-piper-backend",level:2},{value:"Build WasmEdge with WASI-NN Whisper Backend",id:"build-wasmedge-with-wasi-nn-whisper-backend",level:2},{value:"Build WasmEdge with WASI-NN ChatTTS Backend",id:"build-wasmedge-with-wasi-nn-chattts-backend",level:2},{value:"Build WasmEdge with WASI-NN MLX Backend",id:"build-wasmedge-with-wasi-nn-mlx-backend",level:2},{value:"Build WasmEdge with WASI-NN BitNet Backend",id:"build-wasmedge-with-wasi-nn-bitnet-backend",level:2},{value:"Build for ARM with TL1 Optimization",id:"build-for-arm-with-tl1-optimization",level:3},{value:"Build for x86 with TL2 Optimization",id:"build-for-x86-with-tl2-optimization",level:3}],s={toc:p},u="wrapper";function m(e){let{components:t,...a}=e;return(0,l.kt)(u,(0,n.Z)({},s,a,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"build-with-wasi-nn-plug-in"},"Build with WASI-NN Plug-in"),(0,l.kt)("p",null,"The WASI-NN plug-in is a proposed WebAssembly System Interface (WASI) API for machine learning. It allows WebAssembly programs to access host-provided machine learning functions."),(0,l.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,l.kt)("p",null,"Currently, WasmEdge supports following backends for WASI-NN proposal:"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Backend"),(0,l.kt)("th",{parentName:"tr",align:null},"Dependency"),(0,l.kt)("th",{parentName:"tr",align:null},"CMake Option"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"#build-wasmedge-with-wasi-nn-openvino-backend"},"OpenVINO")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://docs.openvino.ai/2023.0/openvino_docs_install_guides_installing_openvino_apt.html"},"OpenVINO\u2122 (2023)")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-DWASMEDGE_PLUGIN_WASI_NN_BACKEND=OpenVINO"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"#build-wasmedge-with-wasi-nn-tensorflow-lite-backend"},"TensorFlow-Lite")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://www.tensorflow.org/install/lang_c"},"TensorFlow Lite")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-DWASMEDGE_PLUGIN_WASI_NN_BACKEND=TensorFlowLite"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"#build-wasmedge-with-wasi-nn-pytorch-backend"},"PyTorch")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://pytorch.org/get-started/locally/"},"PyTorch 1.8.2 LTS")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-DWASMEDGE_PLUGIN_WASI_NN_BACKEND=PyTorch"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"#build-wasmedge-with-wasi-nn-pytorch-backend"},"GGML")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://github.com/ggerganov/llama.cpp"},"llama.cpp")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-DWASMEDGE_PLUGIN_WASI_NN_BACKEND=GGML"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"#build-wasmedge-with-wasi-nn-piper-backend"},"Piper")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://github.com/rhasspy/piper"},"Piper")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-DWASMEDGE_PLUGIN_WASI_NN_BACKEND=Piper"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"#build-wasmedge-with-wasi-nn-whisper-backend"},"Whisper")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://github.com/ggerganov/whisper.cpp"},"whisper.cpp")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-DWASMEDGE_PLUGIN_WASI_NN_BACKEND=Whisper"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"#build-wasmedge-with-wasi-nn-chattts-backend"},"ChatTTS")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://github.com/2noise/ChatTTS"},"ChatTTS")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-DWASMEDGE_PLUGIN_WASI_NN_BACKEND=ChatTTS"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"#build-wasmedge-with-wasi-nn-mlx-backend"},"MLX")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://github.com/ml-explore/mlx"},"MLX")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-DWASMEDGE_PLUGIN_WASI_NN_BACKEND=MLX"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"#build-wasmedge-with-wasi-nn-bitnet-backend"},"BitNet")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://github.com/microsoft/BitNet"},"BitNet.cpp")),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-DWASMEDGE_PLUGIN_WASI_NN_BACKEND=BitNet"))))),(0,l.kt)("p",null,"Developers can ",(0,l.kt)("a",{parentName:"p",href:"/docs/contribute/source/os/linux"},"build the WasmEdge from source")," with the cmake option ",(0,l.kt)("inlineCode",{parentName:"p"},"WASMEDGE_PLUGIN_WASI_NN_BACKEND")," to enable the backends. For supporting multiple backends, developers can assign the option such as ",(0,l.kt)("inlineCode",{parentName:"p"},'-DWASMEDGE_PLUGIN_WASI_NN_BACKEND="GGML;Whisper;TensorFlowLite"'),"."),(0,l.kt)("p",null,"After building, you will have the WASI-NN with specified backend(s) plug-in shared library under ",(0,l.kt)("inlineCode",{parentName:"p"},"<YOUR_BUILD_FOLDER>/plugins/wasi_nn/libwasmedgePluginWasiNN.so")," (or ",(0,l.kt)("inlineCode",{parentName:"p"},".dylib")," extension on MacOS)."),(0,l.kt)("admonition",{type:"note"},(0,l.kt)("p",{parentName:"admonition"},"If the ",(0,l.kt)("inlineCode",{parentName:"p"},"wasmedge")," CLI tool cannot find the WASI-NN plug-in, you can set the ",(0,l.kt)("inlineCode",{parentName:"p"},"WASMEDGE_PLUGIN_PATH")," environment variable to the plug-in installation path (such as ",(0,l.kt)("inlineCode",{parentName:"p"},"/usr/local/lib/wasmedge/"),", or the built plug-in path ",(0,l.kt)("inlineCode",{parentName:"p"},"build/plugins/wasi_nn/"),") to try to fix this issue.")),(0,l.kt)("p",null,"For the ",(0,l.kt)("inlineCode",{parentName:"p"},"Burn.rs")," backend, please use the cmake option ",(0,l.kt)("inlineCode",{parentName:"p"},"WASMEDGE_PLUGIN_WASI_NN_BURNRS_MODEL")," to assign the model."),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Model for ",(0,l.kt)("inlineCode",{parentName:"th"},"Burn.rs")," backend"),(0,l.kt)("th",{parentName:"tr",align:null},"CMake Option"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Squeezenet"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-WASMEDGE_PLUGIN_WASI_NN_BURNRS_MODEL=Squeezenet"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Whisper"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"-WASMEDGE_PLUGIN_WASI_NN_BURNRS_MODEL=Whisper"))))),(0,l.kt)("p",null,"After building, you will have the WASI-NN with specified backend(s) plug-in shared library under ",(0,l.kt)("inlineCode",{parentName:"p"},"<YOUR_BUILD_FOLDER>/plugins/wasi_nn_burnrs/libwasmedgePluginWasiNN.so")," (or ",(0,l.kt)("inlineCode",{parentName:"p"},".dylib")," extension on MacOS)."),(0,l.kt)("admonition",{type:"note"},(0,l.kt)("p",{parentName:"admonition"},"The ",(0,l.kt)("inlineCode",{parentName:"p"},"WASI-NN Burn.rs")," backend cannot build with other backends.")),(0,l.kt)("h2",{id:"build-wasmedge-with-wasi-nn-openvino-backend"},"Build WasmEdge with WASI-NN OpenVINO Backend"),(0,l.kt)("p",null,"For choosing and installing OpenVINO\u2122 on ",(0,l.kt)("inlineCode",{parentName:"p"},"Ubuntu 20.04")," for the backend, we recommend the following commands:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'wget https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\nsudo apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\necho "deb https://apt.repos.intel.com/openvino/2023 ubuntu20 main" | sudo tee /etc/apt/sources.list.d/intel-openvino-2023.list\nsudo apt update\nsudo apt-get -y install openvino\nldconfig\n')),(0,l.kt)("p",null,"Then build and install WasmEdge from source:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="OpenVINO"\ncmake --build build\n')),(0,l.kt)("h2",{id:"build-wasmedge-with-wasi-nn-pytorch-backend"},"Build WasmEdge with WASI-NN PyTorch Backend"),(0,l.kt)("p",null,"For choosing and installing PyTorch on ",(0,l.kt)("inlineCode",{parentName:"p"},"Ubuntu 20.04")," for the backend, we recommend the following commands:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'export PYTORCH_VERSION="1.8.2"\ncurl -s -L -O --remote-name-all https://download.pytorch.org/libtorch/lts/1.8/cpu/libtorch-cxx11-abi-shared-with-deps-${PYTORCH_VERSION}%2Bcpu.zip\nunzip -q "libtorch-cxx11-abi-shared-with-deps-${PYTORCH_VERSION}%2Bcpu.zip"\nrm -f "libtorch-cxx11-abi-shared-with-deps-${PYTORCH_VERSION}%2Bcpu.zip"\nexport LD_LIBRARY_PATH=$(pwd)/libtorch/lib:${LD_LIBRARY_PATH}\nexport Torch_DIR=$(pwd)/libtorch\n')),(0,l.kt)("p",null,"For the legacy operating system such as ",(0,l.kt)("inlineCode",{parentName:"p"},"CentOS 7.6"),", please use the ",(0,l.kt)("inlineCode",{parentName:"p"},"pre-cxx11-abi")," version of ",(0,l.kt)("inlineCode",{parentName:"p"},"libtorch")," instead:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'export PYTORCH_VERSION="1.8.2"\ncurl -s -L -O --remote-name-all https://download.pytorch.org/libtorch/lts/1.8/cpu/libtorch-shared-with-deps-${PYTORCH_VERSION}%2Bcpu.zip\nunzip -q "libtorch-shared-with-deps-${PYTORCH_VERSION}%2Bcpu.zip"\nrm -f "libtorch-shared-with-deps-${PYTORCH_VERSION}%2Bcpu.zip"\nexport LD_LIBRARY_PATH=$(pwd)/libtorch/lib:${LD_LIBRARY_PATH}\nexport Torch_DIR=$(pwd)/libtorch\n')),(0,l.kt)("p",null,"The PyTorch library will be extracted in the current directory ",(0,l.kt)("inlineCode",{parentName:"p"},"./libtorch"),"."),(0,l.kt)("p",null,"Then build and install WasmEdge from source:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="PyTorch"\ncmake --build build\n')),(0,l.kt)("h2",{id:"build-wasmedge-with-wasi-nn-tensorflow-lite-backend"},"Build WasmEdge with WASI-NN TensorFlow-Lite Backend"),(0,l.kt)("p",null,"You can build and install WasmEdge from source directly (on ",(0,l.kt)("inlineCode",{parentName:"p"},"Linux x86_64"),", ",(0,l.kt)("inlineCode",{parentName:"p"},"Linux aarch64"),", ",(0,l.kt)("inlineCode",{parentName:"p"},"MacOS x86_64"),", or ",(0,l.kt)("inlineCode",{parentName:"p"},"MacOS arm64")," platforms):"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="TensorflowLite"\ncmake --build build\n')),(0,l.kt)("p",null,"Installing the necessary ",(0,l.kt)("inlineCode",{parentName:"p"},"libtensorflowlite_c.so")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"libtensorflowlite_flex.so")," on both ",(0,l.kt)("inlineCode",{parentName:"p"},"Ubuntu 20.04")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"manylinux2014")," for the backend, we recommend the following commands:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"curl -s -L -O --remote-name-all https://github.com/second-state/WasmEdge-tensorflow-deps/releases/download/TF-2.12.0-CC/WasmEdge-tensorflow-deps-TFLite-TF-2.12.0-CC-manylinux2014_x86_64.tar.gz\ntar -zxf WasmEdge-tensorflow-deps-TFLite-TF-2.12.0-CC-manylinux2014_x86_64.tar.gz\nrm -f WasmEdge-tensorflow-deps-TFLite-TF-2.12.0-CC-manylinux2014_x86_64.tar.gz\n")),(0,l.kt)("p",null,"The shared library will be extracted in the current directory ",(0,l.kt)("inlineCode",{parentName:"p"},"./libtensorflowlite_c.so")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"./libtensorflowlite_flex.so"),"."),(0,l.kt)("p",null,"Then you can move the library to the installation path:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"mv libtensorflowlite_c.so /usr/local/lib\nmv libtensorflowlite_flex.so /usr/local/lib\n")),(0,l.kt)("p",null,"Or set the environment variable ",(0,l.kt)("inlineCode",{parentName:"p"},"export LD_LIBRARY_PATH=$(pwd):${LD_LIBRARY_PATH}"),"."),(0,l.kt)("admonition",{type:"note"},(0,l.kt)("p",{parentName:"admonition"},"We also provided the ",(0,l.kt)("inlineCode",{parentName:"p"},"darwin_x86_64"),", ",(0,l.kt)("inlineCode",{parentName:"p"},"darwin_arm64"),", and ",(0,l.kt)("inlineCode",{parentName:"p"},"manylinux_aarch64")," versions of the TensorFlow-Lite pre-built shared libraries.")),(0,l.kt)("p",null,"For more information, you can refer to the ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/WasmEdge/WasmEdge/tree/master/plugins/wasi_nn"},"GitHub repository"),"."),(0,l.kt)("h2",{id:"build-wasmedge-with-wasi-nn-llamacpp-backend"},"Build WasmEdge with WASI-NN llama.cpp Backend"),(0,l.kt)("p",null,"You don't need to install any llama.cpp libraries. WasmEdge will download it during the building period."),(0,l.kt)("p",null,"Due to the acceleration frameworks being various, you will need to use different compilation options to build this plugin. Please make sure you are following the same OS section to do this."),(0,l.kt)("h3",{id:"build-with-llamacpp-backend-on-macos"},"Build with llama.cpp Backend on MacOS"),(0,l.kt)("h4",{id:"intel-model"},"Intel Model"),(0,l.kt)("p",null,"If you are using the Intel Model macOS, we won't enable any acceleration framework. It is a pure CPU mode plugin."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\n# Disable BLAS and METAL on x86_64 macOS.\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release \\\n  -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="GGML" \\\n  -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_METAL=OFF \\\n  -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_BLAS=OFF \\\n  .\ncmake --build build\n')),(0,l.kt)("h4",{id:"apple-silicon-model"},"Apple Silicon Model"),(0,l.kt)("p",null,"You can build and install WasmEdge from source directly on the macOS arm64 platform. It will use the built-in GPU by default."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\n# Enable METAL on arm64 macOS.\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release \\\n  -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="GGML" \\\n  -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_METAL=ON \\\n  -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_BLAS=OFF \\\n  .\ncmake --build build\n')),(0,l.kt)("h3",{id:"build-with-llamacpp-backend-on-linux"},"Build with llama.cpp Backend on Linux"),(0,l.kt)("h4",{id:"ubuntudebian-with-cuda-12"},"Ubuntu/Debian with CUDA 12"),(0,l.kt)("p",null,"Please follow the official guide provided by NVIDIA for installing the CUDA framework: ",(0,l.kt)("a",{parentName:"p",href:"https://developer.nvidia.com/cuda-12-2-0-download-archive"},"https://developer.nvidia.com/cuda-12-2-0-download-archive")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\n\n# You may need to install dependencies\napt update\napt install -y software-properties-common lsb-release \\\n  cmake unzip pkg-config\n\n# Due to cuda-related files, it will produce some warning.\n# Disable the warning as an error to avoid failures.\nexport CXXFLAGS="-Wno-error"\n# Please make sure you set up the correct CUDAARCHS.\n# We use `60;61;70` for maximum compatibility.\nexport CUDAARCHS="60;61;70"\n\n# BLAS cannot work with CUBLAS\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_CUDA_ARCHITECTURES="60;61;70" \\\n  -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \\\n  -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="GGML" \\\n  -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_BLAS=OFF \\\n  -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_CUBLAS=ON \\\n  .\n\ncmake --build build\n')),(0,l.kt)("h4",{id:"ubuntu-on-nvidia-jetson-agx-orin"},"Ubuntu on NVIDIA Jetson AGX Orin"),(0,l.kt)("p",null,"You should use the pre-built OS image from the NVIDIA official site."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\n\n# Due to cuda-related files, it will produce some warning.\n# Disable the warning as an error to avoid failures.\nexport CXXFLAGS="-Wno-error"\n# Please make sure you set up the correct CUDAARCHS.\n# 72 is for NVIDIA Jetson AGX Orin\nexport CUDAARCHS=72\n\n# BLAS cannot work with CUBLAS\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \\\n  -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="GGML" \\\n  -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_BLAS=OFF \\\n  -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_CUBLAS=ON \\\n  .\n\ncmake --build build\n')),(0,l.kt)("h4",{id:"ubuntudebian-with-openblas"},"Ubuntu/Debian with OpenBLAS"),(0,l.kt)("p",null,"Please install OpenBLAS before building the plugin."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\n\n# You may need to install dependencies\napt update\napt install -y software-properties-common lsb-release \\\n  cmake unzip pkg-config\n# You must install OpenBLAS\napt install libopenblas-dev\n\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release \\\n  -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="GGML" \\\n  -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_BLAS=ON \\\n  .\n\ncmake --build build\n')),(0,l.kt)("h4",{id:"general-linux-without-any-acceleration-framework"},"General Linux without any acceleration framework"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\n\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release \\\n  -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="GGML" \\\n  -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_BLAS=OFF \\\n  .\n\ncmake --build build\n')),(0,l.kt)("h3",{id:"build-with-llamacpp-backend-on-windows"},"Build with llama.cpp Backend on Windows"),(0,l.kt)("h4",{id:"install-dependencies-for-llamacpp-and-build-on-windows"},"Install Dependencies for llama.cpp And Build on Windows"),(0,l.kt)("p",null,"Developers can follow the steps for installing the requested dependencies."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"(Optional, skip this deps if you don't need to use GPU) Download and install CUDA toolkit"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"We use CUDA Toolkit 12 for the release assets"),(0,l.kt)("li",{parentName:"ul"},"Link: ",(0,l.kt)("a",{parentName:"li",href:"https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local"},"https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local")))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Download and install Visual Studio 2022 Community Edition"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Link: ",(0,l.kt)("a",{parentName:"li",href:"https://visualstudio.microsoft.com/vs/community/"},"https://visualstudio.microsoft.com/vs/community/")),(0,l.kt)("li",{parentName:"ul"},"Select the following components in the installer:",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"msvc v143 - vs 2022 c++ x64/x86 build tools (latest)"),(0,l.kt)("li",{parentName:"ul"},"windows 11 sdk (10.0.22621.0)"),(0,l.kt)("li",{parentName:"ul"},"C++ ATL for v143 build tools (x86 & x64)"))))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Download and install cmake"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"We use cmake 3.29.3 for the release assets"),(0,l.kt)("li",{parentName:"ul"},"Link: ",(0,l.kt)("a",{parentName:"li",href:"https://github.com/Kitware/CMake/releases/download/v3.29.3/cmake-3.29.3-windows-x86_64.msi"},"https://github.com/Kitware/CMake/releases/download/v3.29.3/cmake-3.29.3-windows-x86_64.msi")))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Download and install git"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"We use git 2.45.1"),(0,l.kt)("li",{parentName:"ul"},"Link: ",(0,l.kt)("a",{parentName:"li",href:"https://github.com/git-for-windows/git/releases/download/v2.45.1.windows.1/Git-2.45.1-64-bit.exe"},"https://github.com/git-for-windows/git/releases/download/v2.45.1.windows.1/Git-2.45.1-64-bit.exe")))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Download and install ninja-build"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"We use ninja-build 1.12.1"),(0,l.kt)("li",{parentName:"ul"},"Link: ",(0,l.kt)("a",{parentName:"li",href:"https://github.com/ninja-build/ninja/releases/download/v1.12.1/ninja-win.zip"},"https://github.com/ninja-build/ninja/releases/download/v1.12.1/ninja-win.zip")),(0,l.kt)("li",{parentName:"ul"},"Installation: just unzip it to a custom folder")))),(0,l.kt)("p",null,"Then developers can build by following the steps."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Open Developer PowerShell for VS 2022"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Start -> Visual Studio 2022 -> Visual Studio Tools -> Developer PowerShell for VS 2022"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Inside the PowerShell, use git to download wasmedge repo"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-console"},"cd $HOME\ngit clone https://github.com/WasmEdge/WasmEdge.git\ncd WasmEdge\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Compile wasmedge with enabling the ",(0,l.kt)("inlineCode",{parentName:"p"},"wasi_nn_ggml")," related options, please use the following commands. To build the plugin, you don't need to enable AOT/LLVM related features, so set them to OFF."),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"If you want to enable CUDA:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-console"},'# CUDA ENABLE:\n& "C:\\Program files\\CMake\\bin\\cmake.exe" -Bbuild -GNinja -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_PLUGIN_WASI_NN_BACKEND=ggml -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_CUBLAS=ON -DWASMEDGE_USE_LLVM=OFF .\n& "<the ninja-build folder>\\ninja.exe" -C build\n'))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"If you want to disable CUDA:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-console"},'# CUDA DISABLE:\n& "C:\\Program files\\CMake\\bin\\cmake.exe" -Bbuild -GNinja -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_PLUGIN_WASI_NN_BACKEND=ggml -DWASMEDGE_USE_LLVM=OFF .\n& "<the ninja-build folder>\\ninja.exe" -C build\n'))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"If you want to enable HIP (AMD GPU):"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-console"},'# HIP ENABLE:\n& "C:\\Program files\\CMake\\bin\\cmake.exe" -Bbuild -GNinja -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_PLUGIN_WASI_NN_BACKEND=ggml -DWASMEDGE_PLUGIN_WASI_NN_GGML_LLAMA_HIP=ON -DWASMEDGE_USE_LLVM=OFF .\n& "<the ninja-build folder>\\ninja.exe" -C build\n')))))),(0,l.kt)("h4",{id:"execute-the-wasi-nn-plugin-with-the-llama-example-on-windows"},"Execute the WASI-NN plugin with the llama example on Windows"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Set the environment variables"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-console"},'$env:PATH += ";$pwd\\build\\lib\\api"\n$env:WASMEDGE_PLUGIN_PATH = "$pwd\\build\\plugins"\n'))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Download the wasm and run"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-console"},"wget https://github.com/second-state/WasmEdge-WASINN-examples/raw/master/wasmedge-ggml/llama/wasmedge-ggml-llama.wasm\nwget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/blob/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf\nwasmedge --dir .:. --env llama3=true --env n_gpu_layers=100 --nn-preload default:GGML:AUTO:Meta-Llama-3-8B-Instruct.Q5_K_M.gguf wasmedge-ggml-llama.wasm default\n")))),(0,l.kt)("h4",{id:"troubleshooting-amd-radeon-integrated-graphics-windows"},"Troubleshooting: AMD Radeon Integrated Graphics (Windows)"),(0,l.kt)("p",null,"If you are building the GGML backend on Windows with an integrated AMD GPU (e.g., Radeon 780M / gfx1103) and encounter ",(0,l.kt)("inlineCode",{parentName:"p"},"rocBLAS")," or initialization errors, you may need the following workarounds:"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Set the Architecture Override:"),"\nThe RDNA 3 integrated graphics require an override to match the available ROCm kernels."),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-console"},'$env:HSA_OVERRIDE_GFX_VERSION = "11.0.0"\n'))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Manual Library Linking (ROCm 6.x):")," The ROCm installer on Windows may not create the necessary symlinks for ",(0,l.kt)("inlineCode",{parentName:"p"},"gfx1103"),". If you see errors related to missing ",(0,l.kt)("inlineCode",{parentName:"p"},"TensileLibrary")," files:"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Navigate to your ROCm installation folder (e.g., ",(0,l.kt)("inlineCode",{parentName:"li"},"C:\\Program Files\\AMD\\ROCm\\6.x\\bin\\rocblas\\library"),")."),(0,l.kt)("li",{parentName:"ul"},"Copy the ",(0,l.kt)("inlineCode",{parentName:"li"},"gfx1100")," files and rename them to ",(0,l.kt)("inlineCode",{parentName:"li"},"gfx1103"),".",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"Example: Copy ",(0,l.kt)("inlineCode",{parentName:"li"},"TensileLibrary_lazy_gfx1100.dat")," -> ",(0,l.kt)("inlineCode",{parentName:"li"},"TensileLibrary_lazy_gfx1103.dat"))))))),(0,l.kt)("h3",{id:"appendix-for-llamacpp-backend"},"Appendix for llama.cpp backend"),(0,l.kt)("p",null,"We also provided the pre-built ggml plugins on the following platforms:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"darwin","_","x86","_","64: Intel Model macOS"),(0,l.kt)("li",{parentName:"ul"},"darwin","_","arm64: Apple Silicon Model macOS"),(0,l.kt)("li",{parentName:"ul"},"ubuntu20.04","_","x86","_","64: x86","_","64 Linux (the glibc is using Ubuntu20.04 one)"),(0,l.kt)("li",{parentName:"ul"},"ubuntu20.04","_","aarch64: aarch64 Linux (the glibc is using Ubuntu20.04 one)"),(0,l.kt)("li",{parentName:"ul"},"ubuntu20.04","_","cuda","_","x86","_","64: x86","_","64 Linux with CUDA 12 support (the glibc is using Ubuntu20.04 one)"),(0,l.kt)("li",{parentName:"ul"},"ubuntu20.04","_","cuda","_","aarch64: aarch64 Linux with CUDA 11 support (the glibc is using Ubuntu20.04 one), for NVIDIA Jetson AGX Orin"),(0,l.kt)("li",{parentName:"ul"},"manylinux2014","_","x86","_","64: x86","_","64 Linux (the glibc is using CentOS 7 one)"),(0,l.kt)("li",{parentName:"ul"},"manylinux2014","_","aarch64: aarch64 Linux (the glibc is using CentOS 7 one)")),(0,l.kt)("h2",{id:"build-wasmedge-with-wasi-nn-piper-backend"},"Build WasmEdge with WASI-NN Piper Backend"),(0,l.kt)("p",null,"Build and install WasmEdge from source:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="Piper"\ncmake --build build\n')),(0,l.kt)("h2",{id:"build-wasmedge-with-wasi-nn-whisper-backend"},"Build WasmEdge with WASI-NN Whisper Backend"),(0,l.kt)("p",null,"Build and install WasmEdge from source:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="Whisper"\ncmake --build build\n')),(0,l.kt)("h2",{id:"build-wasmedge-with-wasi-nn-chattts-backend"},"Build WasmEdge with WASI-NN ChatTTS Backend"),(0,l.kt)("p",null,"The ChatTTS backend relies on ChatTTS and Python library, we recommend the following commands to install dependencies."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt update\nsudo apt upgrade\nsudo apt install python3-dev\npip install chattts==0.1.1\n")),(0,l.kt)("p",null,"Then build and install WasmEdge from source:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\n\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="chatTTS"\ncmake --build build\n')),(0,l.kt)("h2",{id:"build-wasmedge-with-wasi-nn-mlx-backend"},"Build WasmEdge with WASI-NN MLX Backend"),(0,l.kt)("p",null,"You can directly build and install WasmEdge from source or custom install mlx and set ",(0,l.kt)("inlineCode",{parentName:"p"},"CMAKE_INSTALL_PREFIX")," variable."),(0,l.kt)("p",null,"Build and install WasmEdge from source:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\n\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="mlx"\ncmake --build build\n')),(0,l.kt)("h2",{id:"build-wasmedge-with-wasi-nn-bitnet-backend"},"Build WasmEdge with WASI-NN BitNet Backend"),(0,l.kt)("p",null,"To build this backend, exactly one of the following target-level (TL) optimization flags must be enabled. Enabling both flags simultaneously is not supported and will result in a build failure. If these specific optimizations are not required, we recommend using the ",(0,l.kt)("inlineCode",{parentName:"p"},"GGML")," backend with ",(0,l.kt)("inlineCode",{parentName:"p"},"llama.cpp")," for broader compatibility."),(0,l.kt)("h3",{id:"build-for-arm-with-tl1-optimization"},"Build for ARM with TL1 Optimization"),(0,l.kt)("p",null,"For ARM-based systems, use the ",(0,l.kt)("inlineCode",{parentName:"p"},"WASMEDGE_PLUGIN_WASI_NN_BITNET_ARM_TL1")," flag. "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release \\\n  -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="BitNet" \\\n  -DWASMEDGE_PLUGIN_WASI_NN_BITNET_ARM_TL1=ON\ncmake --build build\n')),(0,l.kt)("h3",{id:"build-for-x86-with-tl2-optimization"},"Build for x86 with TL2 Optimization"),(0,l.kt)("p",null,"For x86-based systems, use the ",(0,l.kt)("inlineCode",{parentName:"p"},"WASMEDGE_PLUGIN_WASI_NN_BITNET_X86_TL2")," flag."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'cd <path/to/your/wasmedge/source/folder>\ncmake -GNinja -Bbuild -DCMAKE_BUILD_TYPE=Release \\\n  -DWASMEDGE_PLUGIN_WASI_NN_BACKEND="BitNet" \\\n  -DWASMEDGE_PLUGIN_WASI_NN_BITNET_X86_TL2=ON\ncmake --build build\n')))}m.isMDXComponent=!0}}]);